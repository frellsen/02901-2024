{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise 3: Energy based models (solutions)\n",
    "Exercise by [Jes Frellsen](https://frellsen.org) (Technical University of Denmark), August 2024 (version 1.0).\n",
    "\n",
    "The main task in this programming exercise, is to learn and sample from an EBM based on two simple 2D toy datasets. We have provided you with a file for the toy data:\n",
    "* `ToyData.py` contains the code for generating data from the two toy models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can download `ToyData.py` using the following commands:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! curl -O https://raw.githubusercontent.com/frellsen/02901-2024/main/ToyData.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy data\n",
    "First we visualize the probability densities for the toy datasets.\n",
    "\n",
    "When we create an object of the `Chequerboard` or `TwoGaussian`, we can call the forward method which returns a `Distribution` object from `torch.distributions`. The `Distribution` class implements a method for calculating the log probability (`log_prob(...)`), which we will use to make the plots below, and a method for sampling from the distribution (`sample(...)`), which we will later use for creating our training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import ToyData\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Make a density plot of the Checkerboard distribution\n",
    "toy = ToyData.Chequerboard()\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "im = ax1.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax1.set_xlim(toy.xlim)\n",
    "ax1.set_ylim(toy.ylim)\n",
    "ax1.set_aspect('equal')\n",
    "cbar1 = fig.colorbar(im, ax=ax1)\n",
    "ax1.set_title('Checkerboard distribution')\n",
    "cbar1.set_label('Probability density')\n",
    "\n",
    "# Make a density plot of the Gaussian distribution\n",
    "toy = ToyData.TwoGaussians()\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "\n",
    "im = ax2.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax2.set_xlim(toy.xlim)\n",
    "ax2.set_ylim(toy.ylim)\n",
    "ax2.set_aspect('equal')\n",
    "ax2.set_title('Two Gaussians distribution')\n",
    "cbar2 = fig.colorbar(im, ax=ax2)\n",
    "cbar2.set_label('Probability density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.1: Langevin diffusion\n",
    "\n",
    "First you will implement Langevin diffusion to sample from the toy distribution (without the Metropolis correction).\n",
    "\n",
    "Complete the function below, but adding the first-order Euler discretization step $\\mathbf{x}_{t+1} = \\mathbf{x}_{t} + \\frac{\\epsilon^2}{2} \\nabla_\\mathbf{x} \\log p_\\theta(\\mathbf{x}) + \\epsilon \\mathbf{z}$, where $\\epsilon^2$ is the stepsize and $\\mathbf{z} \\sim \\mathcal{N}(0,\\mathbf{I})$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def langevin_dynamics(log_prob, x0, n_steps, stepsize):\n",
    "    \"\"\"\n",
    "    Perform Langevin dynamics to sample from a distribution defined by log_prob.\n",
    "    \n",
    "    Args:\n",
    "    log_prob: A function that takes a sample and returns the unnormalise log-probability of the sample.\n",
    "    x0: The starting sample.\n",
    "    n_steps: The number of steps to run Langevin dynamics.\n",
    "    stepsize: The step size to use for Langevin dynamics.\n",
    "    \"\"\"\n",
    "    x = x0.clone().detach().requires_grad_(True)\n",
    "    samples = []\n",
    "    for i in range(n_steps):\n",
    "        log_prob_val = log_prob(x)\n",
    "        grad = torch.autograd.grad(log_prob_val, x)[0]\n",
    "        x = ... # Add your code here\n",
    "        samples.append(x.clone().detach())\n",
    "    return torch.stack(samples).squeeze()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test you code by sampling from the two Gaussians (the implementation of the chequerboard does not support gradients) and plotting the samples with the true density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from the Checkerboard distribution using Langevin dynamics\n",
    "toy = ToyData.TwoGaussians()\n",
    "log_prob = toy().log_prob\n",
    "x0 = torch.tensor([[0.0, 0.0]])\n",
    "n_steps = 10000\n",
    "stepsize = 0.01\n",
    "noisy_x = langevin_dynamics(log_prob, x0, n_steps, stepsize)\n",
    "\n",
    "# Make a density plot of the toy distribution\n",
    "coordinates = [[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)]\n",
    "prob = torch.exp(toy().log_prob(torch.tensor(coordinates)))\n",
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "im = ax.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "\n",
    "# Make a scatter plot of the samples\n",
    "ax.scatter(noisy_x[:, 0], noisy_x[:, 1], s=1, c='black', alpha=0.5)\n",
    "ax.set_xlim(toy.xlim)\n",
    "ax.set_ylim(toy.ylim)\n",
    "ax.set_aspect('equal')\n",
    "ax.set_title('Samples from the Checkerboard distribution')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.2: MLE of EBMs\n",
    "\n",
    "Next, we will implement MLE based learning for EBMs using the Langevin dynamics sampler."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training loop**: First, we have implemented a generic training loop for learning the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def train(model, optimizer, data_loader, epochs, device):\n",
    "    \"\"\"\n",
    "    Train a Flow model.\n",
    "\n",
    "    Parameters:\n",
    "    model:\n",
    "       The model to train.\n",
    "    optimizer: [torch.optim.Optimizer]\n",
    "         The optimizer to use for training.\n",
    "    data_loader: [torch.utils.data.DataLoader]\n",
    "            The data loader to use for training.\n",
    "    epochs: [int]\n",
    "        Number of epochs to train for.\n",
    "    device: [torch.device]\n",
    "        The device to use for training.\n",
    "    \"\"\"\n",
    "    model.train()\n",
    "\n",
    "    total_steps = len(data_loader)*epochs\n",
    "    progress_bar = tqdm(range(total_steps), desc=\"Training\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        data_iter = iter(data_loader)\n",
    "        for x in data_iter:\n",
    "            if isinstance(x, (list, tuple)):\n",
    "                x = x[0]\n",
    "            x = x.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = model.loss(x)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update progress bar\n",
    "            progress_bar.set_postfix(loss=f\"â €{loss.item():12.4f}\", epoch=f\"{epoch+1}/{epochs}\")\n",
    "            progress_bar.update()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**EBM implementation:** Below we define a simple EMB model class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class EBM_Base(nn.Module):\n",
    "    def __init__(self, energy_fn):\n",
    "        \"\"\"\n",
    "        Base class for Energy-Based Models (EBMs).\n",
    "        \"\"\"\n",
    "        super(EBM_Base, self).__init__()\n",
    "        self.energy_fn = energy_fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Compute the energy of the input samples.\n",
    "        \"\"\"\n",
    "        energy = self.energy_fn(x)\n",
    "        return energy\n",
    "\n",
    "    def log_prob(self, x):\n",
    "        \"\"\"\n",
    "        Compute the unnormalised log probability of the input samples.\n",
    "        \"\"\"\n",
    "        return -self(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**It is you job, to complete the loss function of the `EBM_MLE` class**, based on the expression for the MLE gradients $\\nabla_\\theta \\log p_\\theta(\\mathbf{x}) = -\\nabla_\\theta E_\\theta(\\mathbf{x}) + \\mathbb{E}_{\\mathbf{x}'\\sim p_\\theta(\\mathbf{x}')}[\\nabla_\\theta E_\\theta(\\mathbf{x}')]$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM_MLE(EBM_Base):\n",
    "    def __init__(self, energy_fn):\n",
    "        super(EBM_MLE, self).__init__(energy_fn)\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "        \"\"\"\n",
    "        samples = langevin_dynamics(self.log_prob, x[0], 100, 0.001).detach()\n",
    "        loss = # Add your code here\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Training data**: Next, we generate some training data from the TwoGaussians datasets and create a `data_loader`. We generate a dataset with 10M data points and use a large batch size of 10,000. We can do so, since it is only a two-dimensional dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the data\n",
    "import ToyData\n",
    "\n",
    "batch_size = 100\n",
    "n_data = 500000\n",
    "\n",
    "toy = ToyData.TwoGaussians()\n",
    "train_loader = torch.utils.data.DataLoader(toy().sample((n_data,)), batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(toy().sample((n_data,)), batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Initialize the model and run the training loop**: Finally we initialize the model using a simple fully connected energy function and run the training loop. *Remember that this will not work before you have completed the loss function above.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Get the dimension of the dataset\n",
    "D = next(iter(train_loader)).shape[1]\n",
    "\n",
    "# Define the energy function\n",
    "energy_fn = nn.Sequential(\n",
    "  nn.Linear(D, 64),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(64, 32),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(32, 16),\n",
    "  nn.ReLU(),\n",
    "  nn.Linear(16, 1),\n",
    ")\n",
    "\n",
    "# Define flow model\n",
    "model = EBM_MLE(energy_fn).to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Train model\n",
    "epochs = 1\n",
    "train(model, optimizer, train_loader, epochs, device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Plotting:** Then we plot the learned density and the true density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make two density plots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Get the density of the true distribution\n",
    "coordinates = torch.tensor([[[x,y] for x in np.linspace(*toy.xlim, 1000)] for y in np.linspace(*toy.ylim, 1000)])\n",
    "prob = torch.exp(toy().log_prob(coordinates))\n",
    "\n",
    "# Plot the density of the true distribution\n",
    "im_true = ax1.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax1.set_xlim(toy.xlim)\n",
    "ax1.set_ylim(toy.ylim)\n",
    "ax1.set_aspect('equal')\n",
    "cbar1 = fig.colorbar(im_true, ax=ax1)\n",
    "ax1.set_title('True distribution')\n",
    "cbar1.set_label('Probability density')\n",
    "\n",
    "# Get the density of the EBM\n",
    "prob = torch.exp(model.log_prob(coordinates.float().to(device)).detach())\n",
    "\n",
    "# Plot the density of the EBM\n",
    "im_learned = ax2.imshow(prob, extent=[toy.xlim[0], toy.xlim[1], toy.ylim[0], toy.ylim[1]], origin='lower', cmap='YlOrRd')\n",
    "ax2.set_xlim(toy.xlim)\n",
    "ax2.set_ylim(toy.ylim)\n",
    "ax2.set_aspect('equal')\n",
    "cbar2 = fig.colorbar(im_learned, ax=ax2)\n",
    "ax2.set_title('Learned distribution')\n",
    "cbar2.set_label('Unnormalised density')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.3: Denoising score matching\n",
    "\n",
    "Finally, you will implement learning for EBMs using denoising score matching (DSM) using Gaussian noise with standard deviation `std`.\n",
    "\n",
    "Complete the `loss` method of `EBM_DSM`. Train the model and compared the learned density to the true density."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EBM_DSM(EBM_Base):\n",
    "    def __init__(self, energy_fn):\n",
    "        super(EBM_DSM, self).__init__(energy_fn)\n",
    "\n",
    "    def loss(self, x):\n",
    "        \"\"\"\n",
    "        Compute the loss.\n",
    "        \"\"\"\n",
    "        std = 0.1\n",
    "\n",
    "        losses = []\n",
    "\n",
    "        # Loop over the batch and compute the loss for each sample\n",
    "        for i in range(x.shape[0]):\n",
    "            noisy_x = torch.normal(x[i], std).requires_grad_(True)\n",
    "            stein_score = torch.autograd.grad(self.log_prob(noisy_x), noisy_x, create_graph=True)[0]\n",
    "            noise_score = # Add your code here\n",
    "            loss = # Add your code here\n",
    "            losses.append(loss)\n",
    "\n",
    "        return torch.stack(losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3.4: Chequerboard (optional)\n",
    "\n",
    "Evaluate qualitatively how well the two learning methods work on the chequerboard toy data set.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_02456",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
